{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCJ92oYPJm3Z"
   },
   "source": [
    "# Exploring Machine Intelligence Week 4\n",
    "\n",
    "## Experiments with Image Autoencoders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "we4JLggUMVvB"
   },
   "source": [
    "***Note 1***\n",
    "\n",
    "This notebook was originally written by Vit for the 2020 offering of Exploring Machine Intelligence. Vit in turn borrowed heavily from [https://blog.keras.io/building-autoencoders-in-keras.html](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "\n",
    "***Note 2***\n",
    "\n",
    "It is strongly recommended to run this on colab.google.com if you do not have a GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Set everything up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P1ExBFHWSkoa"
   },
   "outputs": [],
   "source": [
    "# Run this code without any changes\n",
    "# Basic imports\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Input\n",
    "from keras.layers import Reshape, Flatten, BatchNormalization, Activation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "# z = z_mean + sqrt(var) * epsilon\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeQy4OM0ucTd"
   },
   "source": [
    "# Step 1: Load the data using Option 1 or Option 2 below. (Start with Option 1, then move onto Step 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sppEfI7KJm3b"
   },
   "source": [
    "### Option 1: Use MNIST digits (Start here first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZPRLTu6HSks2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n",
      "We loaded the MNIST dataset:\n",
      "input_shape: (28, 28)\n",
      "x_train: (60000, 28, 28, 1)\n",
      "x_test: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "#original_dim = image_size * image_size\n",
    "#x_train = np.reshape(x_train, [-1, original_dim])\n",
    "#x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "input_shape = x_train[0].shape\n",
    "x_train = np.reshape(x_train, x_train.shape+(1,))\n",
    "x_test = np.reshape(x_test, x_test.shape+(1,))\n",
    "\n",
    "print(\"We loaded the MNIST dataset:\")\n",
    "print(\"input_shape:\", input_shape)\n",
    "print(\"x_train:\", x_train.shape)\n",
    "print(\"x_test:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4J-gVgPojaVj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1) stats: 0.99607843 0.0 0.12882152\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at one sample:\n",
    "x1 = x_test[9000] #looks at the 9000th sample\n",
    "print(x1.shape, \"stats:\", np.max(x1), np.min(x1), np.mean(x1))\n",
    "\n",
    "#Show it as an image:\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(x1[:,:,0], cmap='gray', vmin=0.0, vmax=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNy-Jyw8u7B3"
   },
   "source": [
    "### Option 2: Once you've done the whole activity with the MNIST digits dataset, start from here to try it with the QuickDraw dataset\n",
    "\n",
    "We can also use the QuickDraw [dataset](https://github.com/googlecreativelab/quickdraw-dataset) by [@Zaid Alyafeai](https://twitter.com/zaidalyafeai) which was used in the [ML4A guide](https://ml4a.github.io/guides/) for regular AutoEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OyEhhOm4u9KN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'QuickDraw10'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/zaidalyafeai/QuickDraw10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Caqwx8IfvEHf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = np.load('QuickDraw10/dataset/train-ubyte.npz')\n",
    "test_data  = np.load('QuickDraw10/dataset/test-ubyte.npz')\n",
    "\n",
    "x_train, y_train = train_data['a'], test_data['b']\n",
    "x_test,  y_test  = test_data['a'],  test_data['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_tZ0S35UvGI-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 28, 28, 1)\n",
      "(20000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.expand_dims(x_train.astype('float32') / 255., 3)\n",
    "x_test =  np.expand_dims(x_test.astype('float32') / 255. , 3)\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Xzh6c1fH_3r1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1) stats: 1.0 0.0 0.8244548\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at one sample:\n",
    "x1 = x_test[0] # 0 - umbrella\n",
    "print(x1.shape, \"stats:\", np.max(x1), np.min(x1), np.mean(x1))\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(x1[:,:,0], cmap='gray', vmin=0.0, vmax=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that you've loaded the data, you can re-run the rest of the notebook to build a model, interpolate, etc!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UksXjRH1jr1k"
   },
   "source": [
    "# Step 2: Now that you've loaded data (either MNIST or QuickDraw), do this to build the model:\n",
    "\n",
    "Ensure that you've chosen either the MNIST or QuickDraw dataset above, and the dataset you want is now loaded into `x_train` and `x_test`.\n",
    "\n",
    "This code builds a model using a standard convolutional autoencoder architecture:\n",
    "\n",
    "Image -> Encoder -> latent vector representation -> Decoder -> Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAYt9EoQSoO9"
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (28, 28, 1)\n",
    "latent_dim = 32 #you might play with changing this\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "\n",
    "kernels = 26 #you could change this too\n",
    "\n",
    "x = Conv2D(kernels, (3), activation='relu', padding='same')(inputs)\n",
    "x = MaxPooling2D((2), padding='same')(x)\n",
    "x = Conv2D(int(kernels/2), (3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2), padding='same')(x)\n",
    "x = Conv2D(int(kernels/4), (3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2), padding='same')(x)\n",
    "intermediate_conv_shape = x.get_shape()\n",
    "x = Flatten()(x)\n",
    "\n",
    "# optionally BN?\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "_,n,m,o = intermediate_conv_shape # (None, 4, 4, 6) # 96\n",
    "intermediate_dim = n*m*o\n",
    "\n",
    "#some fully connected layers in the middle?\n",
    "#x = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "\n",
    "# optionally BN?\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "x = Reshape((n,m,o))(x)\n",
    "x = Conv2D(int(kernels/4), (3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2))(x)\n",
    "x = Conv2D(int(kernels/2), (3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2))(x)\n",
    "x = Conv2D(int(kernels), (3), activation='relu')(x)\n",
    "x = UpSampling2D((2))(x)\n",
    "outputs = Conv2D(1, (3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUzK4Q2r5YSG"
   },
   "outputs": [],
   "source": [
    "encoder.summary()\n",
    "#plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "decoder.summary()\n",
    "#plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QHpzrXMb582"
   },
   "outputs": [],
   "source": [
    "models = (encoder, decoder)\n",
    "data = (x_test, y_test)\n",
    "\n",
    "args_mse = False\n",
    "# VAE loss = mse_loss or xent_loss + kl_loss\n",
    "if args_mse:\n",
    "    reconstruction_loss = mse(inputs, outputs)\n",
    "else:\n",
    "    reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "\n",
    "m = input_shape[0]*input_shape[1]\n",
    "reconstruction_loss *= m # 28x28 values\n",
    "reconstruction_loss = K.sum(reconstruction_loss)\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae_loss /= m\n",
    "vae_loss /= m\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xG6_I67YjCl"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "#history = vae.fit(x_train[0:1000], epochs=epochs, shuffle=True, batch_size=batch_size, validation_data=(x_test, None))\n",
    "history = vae.fit(x_train, epochs=epochs, shuffle=True, batch_size=batch_size, validation_data=(x_test, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz30ZMO72R26"
   },
   "outputs": [],
   "source": [
    "# How did we do?\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs_array = list(range(epochs))\n",
    "plt.plot(epochs_array, loss, label=\"loss\")\n",
    "plt.plot(epochs_array, val_loss, label=\"val_loss\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"Plot:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYVkB_L7ZfqF"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_model(model, name):\n",
    "    model_json = model.to_json()\n",
    "    with open(name+\".json\", \"w\") as json_file:\n",
    "        json.dump(model_json, json_file)\n",
    "\n",
    "    model.save_weights(name+\".h5\")\n",
    "\n",
    "#you might want to give your model an understandable filename\n",
    "save_model(encoder,'encoder_mnist')\n",
    "save_model(decoder,'decoder_mnist')\n",
    "#save_model(encoder,'encoder_draw_10ep')\n",
    "#save_model(decoder,'decoder_draw_10ep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9fI5yDycZWK"
   },
   "source": [
    "## Now let's use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzcFS5eUnQpW"
   },
   "outputs": [],
   "source": [
    "# We can carry these files (*.h5, *.json) somewhere else ...\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "def my_load_model(name):\n",
    "    with open(name+'.json','r') as f:\n",
    "        model_json = json.load(f)\n",
    "\n",
    "    model = model_from_json(model_json)\n",
    "    model.load_weights(name+'.h5')\n",
    "    return model\n",
    "\n",
    "#Use same filename as above here\n",
    "decoder = my_load_model('decoder_mnist')\n",
    "encoder = my_load_model('encoder_mnist')\n",
    "#decoder = my_load_model('decoder_draw_10ep')\n",
    "#encoder = my_load_model('encoder_draw_10ep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuxxAjg9vj4s"
   },
   "source": [
    "## Inspect one in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANTzSexmcbOO"
   },
   "outputs": [],
   "source": [
    "# Encoded image:\n",
    "\n",
    "x1 = x_test[73] # number 73 picked for no reason at all\n",
    "print(x1.shape)\n",
    "print(np.max(x1), np.min(x1), np.mean(x1))\n",
    "\n",
    "img = x1[:,:,0]\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(img, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtM6GCDXcmZz"
   },
   "outputs": [],
   "source": [
    "# Latent vector:\n",
    "\n",
    "x1_arr = np.asarray([x1])\n",
    "z, z_mean, z_log_var = encoder.predict(x1_arr)\n",
    "print(z.shape)\n",
    "print(np.max(z), np.min(z), np.mean(z))\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(z[0])\n",
    "plt.show()\n",
    "\n",
    "#Plotting is not particularly informative here, it's just a set of 32 numbers, but why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwZcDisYcn7K"
   },
   "outputs": [],
   "source": [
    "# Reconstructed image\n",
    "\n",
    "y1 = decoder.predict(z)\n",
    "print(y1.shape)\n",
    "y1 = y1[0]\n",
    "\n",
    "img = y1[:,:,0]\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(img, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29OmI-qhvnwp"
   },
   "source": [
    "## Or in triplets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWdrzugZo69j"
   },
   "outputs": [],
   "source": [
    "def plot_tripple(image, vector, reconstruction):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "    fig.suptitle('Image > Representation > Reconstruction')\n",
    "    ax1.imshow(image, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "    ax2.plot(vector)\n",
    "    ax2.set_aspect(3.1)\n",
    "    ax3.imshow(reconstruction, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "\n",
    "def plot_single(image):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(image, cmap='gray', vmin=0.0, vmax=1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exteRSHpegnF"
   },
   "outputs": [],
   "source": [
    "x1 = x_test[9] # Randomly pick 9th image from test set\n",
    "z, z_mean, z_log_var = encoder.predict(np.asarray([x1])) \n",
    "y1 = decoder.predict(z)\n",
    "\n",
    "plot_tripple(x1[:,:,0], z[0], y1[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MO8oh8EgJm3j"
   },
   "source": [
    "### Look at a few randomly selected images and compare original, latent vector, and reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GSfZrPB4oRv"
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "for i in range(5):\n",
    "    x1 = x_test[randrange(len(x_test))]\n",
    "    z, z_mean, z_log_var = encoder.predict(np.asarray([x1]))\n",
    "    y1 = decoder.predict(z)\n",
    "    plot_tripple(x1[:,:,0], z[0], y1[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKxt3PC4Jm3j"
   },
   "source": [
    "### Show interpolation/moprhing between two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjznXdDppCsW"
   },
   "outputs": [],
   "source": [
    "sample_a = x_test[2] \n",
    "z_sample_a_encoded, _, _ = encoder.predict(np.asarray([sample_a]))\n",
    "\n",
    "sample_b = x_test[3] \n",
    "z_sample_b_encoded, _, _ = encoder.predict(np.asarray([sample_b]))\n",
    "\n",
    "print(\"z_sample_a_encoded:\", z_sample_a_encoded.shape)\n",
    "print(\"z_sample_b_encoded:\", z_sample_b_encoded.shape)\n",
    "\n",
    "#Let's show both original images \n",
    "plot_single(sample_a.reshape((28,28)))\n",
    "plot_single(sample_b.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncN_5Upp4xGp"
   },
   "outputs": [],
   "source": [
    "# Show the latent vectors for each image\n",
    "plt.plot(z_sample_a_encoded[0])\n",
    "plt.show()\n",
    "plt.plot(z_sample_b_encoded[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDSl4UenprOf"
   },
   "outputs": [],
   "source": [
    "# Define a function to linearly interpolate from one latent vector for another,\n",
    "# specifically a*100% of the way there (i.e., if a=0.25, then we'll be 25% of the way from a to b)\n",
    "def lerp(u,v,a):\n",
    "    # linear interpolation between vectors u and v\n",
    "    return a*u + (1-a)*v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "is6HM7AGqYnZ"
   },
   "outputs": [],
   "source": [
    "a = 0.5  #show one image halfway between a and b\n",
    "z_mix = lerp(z_sample_a_encoded, z_sample_b_encoded, a)\n",
    "image = decoder.predict(z_mix) # shape comes as (1,28,28,1)\n",
    "image = image.reshape((28,28))\n",
    "\n",
    "plot_single(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qmU61nrp1KL"
   },
   "outputs": [],
   "source": [
    "# Now interpolate in a set number of steps\n",
    "steps = 5\n",
    "for i in range(steps + 1):\n",
    "    # Goes from 0.0 to 1.0 in <steps> steps\n",
    "    a_01 = float(i) / float(steps)\n",
    "    z_mix = lerp(z_sample_a_encoded, z_sample_b_encoded, a_01)\n",
    "    y = decoder.predict(z_mix)\n",
    "    image = y.reshape((28,28))\n",
    "    print(a_01,\":\")\n",
    "    plot_single(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufq8zbduB2wj"
   },
   "source": [
    "## Produce a new image from a completely random latent vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xR8vmMyIB5SP"
   },
   "outputs": [],
   "source": [
    "# Random latent\n",
    "latent = np.random.randn(1, 32) * 5\n",
    "print(\"latent = \",latent)\n",
    "\n",
    "image = decoder.predict(latent)\n",
    "image = image.reshape((28,28))\n",
    "\n",
    "plot_single(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSzFK4pMChn7"
   },
   "outputs": [],
   "source": [
    "# We can try to break it ...\n",
    "\n",
    "latent = np.zeros(32)\n",
    "latent[0] = 999.0 # oversaturated?\n",
    "print(\"latent = \",latent)\n",
    "\n",
    "image = decoder.predict(latent.reshape(1,32))\n",
    "image = image.reshape((28,28))\n",
    "\n",
    "plot_single(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucy3KNYCz4t_"
   },
   "source": [
    "## (Or) visualization as a gif:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZs9kqpq1Ate"
   },
   "source": [
    "As a bonus including a visualization from https://github.com/zaidalyafeai/Notebooks/blob/master/AutoEncoders.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you'll need to install `imageio` and `opencv-python` modules.\n",
    "\n",
    "You can do this by running in a terminal/command window:\n",
    "\n",
    "`conda install imageio`\n",
    "\n",
    "`pip install opencv-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3Otq7F2z6j6"
   },
   "outputs": [],
   "source": [
    "import shutil \n",
    "import cv2\n",
    "import glob\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "#linear interpolation function \n",
    "def f(x):\n",
    "  return x\n",
    "\n",
    "def interpolate(size = 10):\n",
    "  if os.path.exists(\"images\"):\n",
    "    shutil.rmtree(\"images\")\n",
    "    os.makedirs('images')\n",
    "  else:\n",
    "    os.makedirs('images')\n",
    "    \n",
    "  \n",
    "  \n",
    "  #get 3 random batches each of size 3 \n",
    "  batches = []\n",
    "  for _ in range(0, 3):\n",
    "    i1 = np.random.randint(0, len(x_train))\n",
    "    i2 = np.random.randint(0, len(x_train))\n",
    "    batches.append([x_train[i1:i1+3], x_train[i2:i2+3]])\n",
    " \n",
    "  i = 0   \n",
    "  for x in list(np.linspace(0, 1, size)):\n",
    "    frame = None\n",
    "    j = 0 \n",
    "    \n",
    "    #interpolate each batch and concatenate them at the end to create 3x3 images\n",
    "    for (x1, x2) in batches:\n",
    "    \n",
    "      \n",
    "      v1,_,_ = encoder.predict(x1) \n",
    "      v2,_,_ = encoder.predict(x2)\n",
    "        \n",
    "      #use a linear interpolater\n",
    "      v = (float(x))*v1 + (1.0 - float(x))*v2\n",
    "      \n",
    "      #get the output and reshape it \n",
    "      y = decoder.predict(v)\n",
    "      img = np.reshape(y, (3 * 28, 28))\n",
    "      img = img * 255\n",
    "      \n",
    "      #concatenate the batches \n",
    "      if frame is None:\n",
    "        frame = img\n",
    "      else:\n",
    "        frame = np.concatenate([frame, img], axis = 1)\n",
    "      j += 1\n",
    "      \n",
    "    #write the current frame to the disk \n",
    "    frame = cv2.resize(frame, (256, 256))  \n",
    "    cv2.imwrite(f'images/image{i}.png', frame)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxMnWyHm0BOS"
   },
   "outputs": [],
   "source": [
    "!mkdir images\n",
    "!ls images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TUsXYTg05gQ"
   },
   "outputs": [],
   "source": [
    "interpolate(size = 10)\n",
    "\n",
    "#with imageio.get_writer('lsi.gif', mode='I', duration=0.35) as writer:\n",
    "#  filenames = glob.glob('images/image*.png')\n",
    "#  filenames = sorted(filenames)\n",
    "#  \n",
    "#  for i,filename in enumerate(filenames):\n",
    "#    image = imageio.imread(filename)\n",
    "#    writer.append_data(image)\n",
    "    \n",
    "# this is a hack to display the gif inside the notebook\n",
    "#os.system('cp lsi.gif lsi.gif.png')\n",
    "\n",
    "\n",
    "png_dir = 'images'\n",
    "images2 = []\n",
    "for file_name in sorted(os.listdir(png_dir)):\n",
    "    if file_name.endswith('.png'):\n",
    "        file_path = os.path.join(png_dir, file_name)\n",
    "        images2.append(imageio.imread(file_path))\n",
    "\n",
    "# Make it pause at the end so that the viewers can ponder\n",
    "for _ in range(10):\n",
    "    images2.append(imageio.imread(file_path))\n",
    "\n",
    "imageio.mimsave('movie.gif', images2)\n",
    "os.system('cp movie.gif movie.gif.png') #To be able to display it in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5Ob-whl3CYJ"
   },
   "outputs": [],
   "source": [
    "from IPython import display \n",
    "display.Image(filename=\"movie.gif.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLu-j3lhS579"
   },
   "source": [
    "## Activities to do on your own\n",
    "1. (Easy) Try modifying the autoencoder or its training in at least one way that gives you a noticeable impact on the results. (e.g., change the optimiser, the number of kernels, the number of layers, the dimensionality of the latent vector, the activation function, etc.) How does this impact on the results? What changes did you try that did not impact the results noticably?\n",
    "2. (More difficult) Try training this autoencoder on a new dataset. For instance, you might grab an existing image dataset from [https://www.kaggle.com/datasets?tags=13207-Computer+Vision](https://www.kaggle.com/datasets?tags=13207-Computer+Vision). \n",
    "   * You will have to tweak the code above if your images are a different size other than 28x28 (it's unwise to use images that are significantly bigger than this, though, unless you are willing to wait a while for training!). For instance, the `input_shape` variable above specifies the input size.\n",
    "   * Further, you'll need to tweak the code if you're using colour images, as this code works with grayscale (i.e., images are 28x28x1). \n",
    "   * [This blog post](https://machinelearningmastery.com/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras/) provides a good introduction to managing your own datasets\n",
    "       * Note that you can use `target_size` to specify the size of the image to read\n",
    "   * If you want to do a batch resize/edit on images within a directory, you might find [tools like this one](https://github.com/dvschultz/dataset-tools) useful \n",
    "3. (More difficult) Try building a version of Mario Klingemann's \"X Degrees of Separation\" for a dataset (could be MNIST or QuickDraw or something else). Write the code to select 5 (or some other number) of existing images that follow a smooth (as smooth as possible) path through latent space between two chosen images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "5.1_convolutional_VAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
